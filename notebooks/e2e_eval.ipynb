{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb103b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This contains the test procedure for vision to weights\n",
    "## Author : Avadesh Meduri\n",
    "## Date : 31/05/2022\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a975a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "python_path = pathlib.Path('.').absolute().parent/'python'\n",
    "os.sys.path.insert(1, str(python_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f223e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is demo for kuka reaching a desired point with diff_qp\n",
    "## Author : Avadesh Meduri\n",
    "## Date : 25/02/2022\n",
    "import time\n",
    "import numpy as np\n",
    "import pinocchio as pin\n",
    "from robot_properties_kuka.config import IiwaConfig\n",
    "from vocam.diff_pin_costs import DiffFrameTranslationCost, DiffFrameVelocityCost\n",
    "\n",
    "import meshcat\n",
    "import meshcat.transformations as tf\n",
    "import meshcat.geometry as g\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, RandomSampler, Sampler\n",
    "import time\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "import numba\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Resize\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vocam.inverse_qp import IOC\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from vocam.forward_pass import IOCForwardPassWithoutVision\n",
    "from vocam.nets import Net\n",
    "\n",
    "from vocam.qpnet import QPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb0fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = IiwaConfig.buildRobotWrapper()\n",
    "model, data = robot.model, robot.data\n",
    "f_id = model.getFrameId(\"EE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c6e9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7001/static/\n"
     ]
    }
   ],
   "source": [
    "viz = pin.visualize.MeshcatVisualizer(robot.model, robot.collision_model, robot.visual_model)\n",
    "viz.initViewer(open=False)\n",
    "viz.loadViewerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ffa228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, fnames, mean = None, std = None, rgbd = True, resize = (224,224)):\n",
    "        \n",
    "        self.rgbd = rgbd\n",
    "        self.resize = resize\n",
    "        self.y_len = [0]\n",
    "        self.img_dir = []\n",
    "        for i in range(len(fnames)):\n",
    "            self.img_dir.append(\"../vision/image_data/data\" + str(fnames[i]))\n",
    "            self.data = np.load(\"../vision/position_data/data\" + str(fnames[i]) + \".npz\")\n",
    "            if i == 0:\n",
    "                self.y_train = torch.tensor(self.data[\"position\"]).float()\n",
    "                self.y_len.append(len(self.data[\"position\"])-1)\n",
    "                \n",
    "            else:\n",
    "                self.y_train = torch.vstack((self.y_train, torch.tensor(self.data[\"position\"]).float()))\n",
    "                self.y_len.append(self.y_len[-1] + len(self.data[\"position\"]))\n",
    "        \n",
    "        if isinstance(mean, np.ndarray) and isinstance(std, np.ndarray):\n",
    "            print(\"using given mean\")\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            self.y_train = (self.y_train - self.mean)/self.std\n",
    "        else:\n",
    "            self.mean = torch.mean(self.y_train, axis = 0)\n",
    "            self.std = torch.std(self.y_train, axis = 0)\n",
    "            print(self.mean, self.std)\n",
    "            self.y_train = (self.y_train - self.mean)/self.std\n",
    "                \n",
    "    def get_data(self, gidx):\n",
    "        \n",
    "        \n",
    "        b_idx = max(np.searchsorted(self.y_len, gidx)-1,0) # which dir to look into\n",
    "        idx = max(gidx - self.y_len[b_idx] - 1,0) # relative idx\n",
    "        \n",
    "#         print(type(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\")))\n",
    "        image = ToTensor()(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\"))\n",
    "        if self.rgbd:\n",
    "            d_image = ToTensor()(imread(self.img_dir[b_idx] + \"/depth_\" + str(idx) + \".jpg\"))\n",
    "            image = torch.vstack((image, d_image))\n",
    "            image = transforms.functional.crop(image,  50, 100, 180, 180)\n",
    "\n",
    "        else:\n",
    "            image = transforms.functional.crop(image,  50, 100, 180, 180)\n",
    "            image = transforms.Resize(self.resize)(image)                    \n",
    "\n",
    "        label = self.y_train[gidx]\n",
    "        \n",
    "        return image.float()[None,:,:,:], label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ba79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_Net_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv11 = nn.Conv2d(4, 64, 3)\n",
    "        self.conv12 = nn.Conv2d(64, 64, 3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv22 = nn.Conv2d(128, 128, 3)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(128, 256, 3)\n",
    "        self.conv32 = nn.Conv2d(256, 256, 3)\n",
    "        self.conv33 = nn.Conv2d(256, 256, 3)\n",
    "\n",
    "        \n",
    "        self.conv41 = nn.Conv2d(256, 512, 3)\n",
    "        self.conv42 = nn.Conv2d(512, 512, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = self.pool(F.relu(self.conv12(x)))\n",
    "        \n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = self.pool(F.relu(self.conv22(x)))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv31(x)))\n",
    "        x = self.pool(F.relu(self.conv32(x)))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv41(x)))\n",
    "        x = F.relu(self.conv42(x))\n",
    "            \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        enc = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11e41178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using given mean\n"
     ]
    }
   ],
   "source": [
    "nq = model.nq\n",
    "nv = model.nv\n",
    "q0 = [np.pi/16.0, -np.pi/16.0, 0, 0, 0, 0, 0]\n",
    "x_init = np.concatenate([q0, pin.utils.zero(model.nv)])\n",
    "\n",
    "n_col = 5\n",
    "u_max = [2.5,2.5,2.5, 1.5, 1.5, 1.5, 1.0]\n",
    "n_vars = 3*nq*n_col+2*nq\n",
    "dt = 0.05\n",
    "\n",
    "isvec = True\n",
    "lr = 1e-1\n",
    "max_eps = 100\n",
    "\n",
    "\n",
    "nn_dir = \"../models/qpnet_91.pt\"\n",
    "net= QPNet(2*nq + 3, 2*n_vars).eval()\n",
    "net.load(nn_dir)\n",
    "\n",
    "# iocfp = IOCForwardPassWithoutVision(net, u_max=u_max)\n",
    "\n",
    "dtc = DiffFrameTranslationCost.apply\n",
    "# for the vision part\n",
    "indices = [1,2,3,4,5,6,7,8,9]\n",
    "mean = np.array([0.3068, 0.1732, 0.4015])\n",
    "std = np.array([0.1402, 0.2325, 0.1624])\n",
    "dl = BoxDataSet(indices, mean = mean, std = std, rgbd = True, resize = (224,224))\n",
    "\n",
    "encoder = C_Net_encoder()\n",
    "encoder.load_state_dict(torch.load(\"../vision/models/cnn5\", map_location=torch.device('cpu')))\n",
    "\n",
    "## EncoderNet\n",
    "nn_dir = \"../models/e2eNet4\"\n",
    "encoder_net= QPNet(2*nq + 512, 2*n_vars).eval()\n",
    "encoder_net.load(nn_dir)\n",
    "iocfp = IOCForwardPassWithoutVision(encoder_net, u_max=u_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4334c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "n_mpc = 25\n",
    "d_tol = 0.5 # how close the ee should be to the ball to be accepted as a good data point\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "q_des_arr = np.array([[2.1789238e-02,  3.3214998e-01, -1.4518893e-04, -8.7141126e-01,\n",
    "                          6.0329604e-01, -1.3965217e-03,  1.4794523e-04],\n",
    "                      [1.3737, 0.9711, 1.6139, 1.2188, 1.5669, 0.1236, 0.2565]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ca9fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index :143/10000 Encoder error : 0.0028\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3151307/422583402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateFramePlacements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/openrobots/lib/python3.8/site-packages/pinocchio/visualize/meshcat_visualizer.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshScale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhomogeneous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;31m# Update viewer configuration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetViewerNodeName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeometryType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVISUAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in range(buffer_size):\n",
    "\n",
    "    if k % n_mpc == 0:\n",
    "                        \n",
    "        with torch.no_grad():\n",
    "            image, label = dl.get_data(np.random.randint(dl.y_train.shape[0]))\n",
    "            pred_loc, encoding = encoder(image)\n",
    "            error = loss(label.unsqueeze(0), pred_loc).numpy()\n",
    "            pred_loc = pred_loc*std + mean\n",
    "            x_des = label*std + mean\n",
    "            x_des[0] += 0.3\n",
    "            \n",
    "        viz.viewer[\"box\"].set_object(g.Sphere(0.05), \n",
    "                         g.MeshLambertMaterial(\n",
    "                             color=0xff22dd,\n",
    "                             reflectivity=0.8))\n",
    "        viz.viewer[\"box\"].set_transform(tf.translation_matrix(x_des.detach().numpy()))\n",
    "        \n",
    "        if np.random.randint(2) == 0 or k == 0:\n",
    "            x_init = np.zeros(2*nq)\n",
    "            x_init[0:nq] = q_des_arr[0] + 0.3*2*(np.random.rand(nq) - 0.5)\n",
    "            x_init[0] -= 2*0.5*(np.random.rand(1) - 0.5)\n",
    "            x_init[2] -= 2*0.3*(np.random.rand(1) - 0.5)\n",
    "            x_init[nq:] = 0.7*2*(np.random.rand(nv) - 0.5)\n",
    "    \n",
    "    else:\n",
    "        x_init = x_pred[3*nq*(n_col-1):3*nq*(n_col-1) + 2*nq]\n",
    "    \n",
    "    x_pred = iocfp.predict_encoder(x_init, encoding)\n",
    "#     x_pred = iocfp.predict(x_init[0:nq], x_init[nq:2*nq], x_des)    \n",
    "    \n",
    "    print(\"Index :\" + str(k) + \"/\" + str(buffer_size) + \" Encoder error : \" + str(np.round(error,4)), end = '\\r', flush = True)\n",
    "    \n",
    "    for i in range(n_col+1):\n",
    "        q = x_pred[3*nq*i:3*nq*i + nq]\n",
    "        dq = x_pred[3*nq*i + nq:3*nq*i + 2*nq]\n",
    "\n",
    "        pin.forwardKinematics(model, data, q, dq, np.zeros(nv))\n",
    "        pin.updateFramePlacements(model, data)\n",
    "\n",
    "        viz.display(q)\n",
    "        time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f3d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

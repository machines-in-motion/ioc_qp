{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2786f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This contains the training procedure for vision to weights\n",
    "## Author : Avadesh Meduri\n",
    "## Date : 31/05/2022\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3e4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "python_path = pathlib.Path('.').absolute().parent/'python'\n",
    "os.sys.path.insert(1, str(python_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de465da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is demo for kuka reaching a desired point with diff_qp\n",
    "## Author : Avadesh Meduri\n",
    "## Date : 25/02/2022\n",
    "import time\n",
    "import numpy as np\n",
    "import pinocchio as pin\n",
    "from robot_properties_kuka.config import IiwaConfig\n",
    "from vocam.diff_pin_costs import DiffFrameTranslationCost, DiffFrameVelocityCost\n",
    "\n",
    "import meshcat\n",
    "import meshcat.transformations as tf\n",
    "import meshcat.geometry as g\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, RandomSampler, Sampler\n",
    "import time\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "import numba\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Resize\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vocam.inverse_qp import IOC\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from vocam.forward_pass import IOCForwardPassWithoutVision\n",
    "from vocam.nets import Net\n",
    "\n",
    "from vocam.qpnet import QPNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a724c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = IiwaConfig.buildRobotWrapper()\n",
    "model, data = robot.model, robot.data\n",
    "f_id = model.getFrameId(\"EE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99124f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7000/static/\n"
     ]
    }
   ],
   "source": [
    "viz = pin.visualize.MeshcatVisualizer(robot.model, robot.collision_model, robot.visual_model)\n",
    "viz.initViewer(open=False)\n",
    "viz.loadViewerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80234186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, fnames, mean = None, std = None, rgbd = True, resize = (224,224)):\n",
    "        \n",
    "        self.rgbd = rgbd\n",
    "        self.resize = resize\n",
    "        self.y_len = [0]\n",
    "        self.img_dir = []\n",
    "        for i in range(len(fnames)):\n",
    "            self.img_dir.append(\"../vision/image_data/data\" + str(fnames[i]))\n",
    "            self.data = np.load(\"../vision/position_data/data\" + str(fnames[i]) + \".npz\")\n",
    "            if i == 0:\n",
    "                self.y_train = torch.tensor(self.data[\"position\"]).float()\n",
    "                self.y_len.append(len(self.data[\"position\"])-1)\n",
    "                \n",
    "            else:\n",
    "                self.y_train = torch.vstack((self.y_train, torch.tensor(self.data[\"position\"]).float()))\n",
    "                self.y_len.append(self.y_len[-1] + len(self.data[\"position\"]))\n",
    "        \n",
    "        if isinstance(mean, np.ndarray) and isinstance(std, np.ndarray):\n",
    "            print(\"using given mean\")\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            self.y_train = (self.y_train - self.mean)/self.std\n",
    "        else:\n",
    "            self.mean = torch.mean(self.y_train, axis = 0)\n",
    "            self.std = torch.std(self.y_train, axis = 0)\n",
    "            print(self.mean, self.std)\n",
    "            self.y_train = (self.y_train - self.mean)/self.std\n",
    "                \n",
    "    def get_data(self, gidx):\n",
    "        \n",
    "        \n",
    "        b_idx = max(np.searchsorted(self.y_len, gidx)-1,0) # which dir to look into\n",
    "        idx = max(gidx - self.y_len[b_idx] - 1,0) # relative idx\n",
    "        \n",
    "#         print(type(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\")))\n",
    "        image = ToTensor()(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\"))\n",
    "        if self.rgbd:\n",
    "            d_image = ToTensor()(imread(self.img_dir[b_idx] + \"/depth_\" + str(idx) + \".jpg\"))\n",
    "            image = torch.vstack((image, d_image))\n",
    "            image = transforms.functional.crop(image,  50, 100, 180, 180)\n",
    "\n",
    "        else:\n",
    "            image = transforms.functional.crop(image,  50, 100, 180, 180)\n",
    "            image = transforms.Resize(self.resize)(image)                    \n",
    "\n",
    "        label = self.y_train[gidx]\n",
    "        \n",
    "        return image.float()[None,:,:,:], label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6dc0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_Net_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv11 = nn.Conv2d(4, 64, 3)\n",
    "        self.conv12 = nn.Conv2d(64, 64, 3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv22 = nn.Conv2d(128, 128, 3)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(128, 256, 3)\n",
    "        self.conv32 = nn.Conv2d(256, 256, 3)\n",
    "        self.conv33 = nn.Conv2d(256, 256, 3)\n",
    "\n",
    "        \n",
    "        self.conv41 = nn.Conv2d(256, 512, 3)\n",
    "        self.conv42 = nn.Conv2d(512, 512, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = self.pool(F.relu(self.conv12(x)))\n",
    "        \n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = self.pool(F.relu(self.conv22(x)))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv31(x)))\n",
    "        x = self.pool(F.relu(self.conv32(x)))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv41(x)))\n",
    "        x = F.relu(self.conv42(x))\n",
    "            \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        enc = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x, enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b9aa639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using given mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq = model.nq\n",
    "nv = model.nv\n",
    "q0 = [np.pi/16.0, -np.pi/16.0, 0, 0, 0, 0, 0]\n",
    "x_init = np.concatenate([q0, pin.utils.zero(model.nv)])\n",
    "\n",
    "n_col = 5\n",
    "u_max = [2.5,2.5,2.5, 1.5, 1.5, 1.5, 1.0]\n",
    "n_vars = 3*nq*n_col+2*nq\n",
    "dt = 0.05\n",
    "\n",
    "isvec = True\n",
    "lr = 1e-1\n",
    "max_eps = 100\n",
    "\n",
    "\n",
    "nn_dir = \"../models/qpnet_89.pt\"\n",
    "net= QPNet(2*nq + 3, 2*n_vars).eval()\n",
    "net.load(nn_dir)\n",
    "\n",
    "iocfp = IOCForwardPassWithoutVision(net, u_max=u_max)\n",
    "\n",
    "dtc = DiffFrameTranslationCost.apply\n",
    "# for the vision part\n",
    "indices = [1,2,3,4,5,6,7,8,9]\n",
    "mean = np.array([0.3068, 0.1732, 0.4015])\n",
    "std = np.array([0.1402, 0.2325, 0.1624])\n",
    "dl = BoxDataSet(indices, mean = mean, std = std, rgbd = True, resize = (224,224))\n",
    "\n",
    "encoder = C_Net_encoder()\n",
    "encoder.load_state_dict(torch.load(\"../vision/models/cnn2\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9b13394",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "f_name = \"15\"\n",
    "n_mpc = 25\n",
    "d_tol = 0.1 # how close the ee should be to the ball to be accepted as a good data point\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "x_train = torch.zeros((1,len(x_init) + 512))\n",
    "x_train_tmp = torch.zeros((n_mpc,len(x_init) + 512))\n",
    "\n",
    "n_vars = 3*nq*n_col + 2*nq\n",
    "if not isvec:\n",
    "    y_train_tmp = torch.zeros((n_mpc, n_vars**2 + n_vars))\n",
    "else:\n",
    "    y_train_tmp = torch.zeros((n_mpc, 2*n_vars))\n",
    "\n",
    "q_des_arr = np.array([[2.1789238e-02,  3.3214998e-01, -1.4518893e-04, -8.7141126e-01,\n",
    "                          6.0329604e-01, -1.3965217e-03,  1.4794523e-04],\n",
    "                      [1.3737, 0.9711, 1.6139, 1.2188, 1.5669, 0.1236, 0.2565]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfec5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index :11/10000 Encoder error : 0.0003 Data Size is : 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ameduri/devel/workspace/dif_ddp/python/vocam/forward_pass.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_input = torch.hstack((torch.tensor(state), torch.tensor(x_des))).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index :2480/10000 Encoder error : 0.0027 Data Size is : 1325\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         pin\u001b[38;5;241m.\u001b[39mforwardKinematics(model, data, q, dq, np\u001b[38;5;241m.\u001b[39mzeros(nv))\n\u001b[1;32m     52\u001b[0m         pin\u001b[38;5;241m.\u001b[39mupdateFramePlacements(model, data)\n\u001b[0;32m---> 54\u001b[0m         \u001b[43mviz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#         time.sleep(0.01)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#     storing x train\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     x_train_tmp[k \u001b[38;5;241m%\u001b[39m n_mpc][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnq] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x_init)\n",
      "File \u001b[0;32m/opt/openrobots/lib/python3.8/site-packages/pinocchio/visualize/meshcat_visualizer.py:289\u001b[0m, in \u001b[0;36mMeshcatVisualizer.display\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdatePlacements(pin\u001b[38;5;241m.\u001b[39mGeometryType\u001b[38;5;241m.\u001b[39mCOLLISION)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_visuals:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdatePlacements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGeometryType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVISUAL\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/openrobots/lib/python3.8/site-packages/pinocchio/visualize/meshcat_visualizer.py:313\u001b[0m, in \u001b[0;36mMeshcatVisualizer.updatePlacements\u001b[0;34m(self, geometry_type)\u001b[0m\n\u001b[1;32m    310\u001b[0m     T \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mhomogeneous\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Update viewer configuration.\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvisual_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mset_transform(T)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/meshcat/visualizer.py:146\u001b[0m, in \u001b[0;36mVisualizer.__getitem__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Visualizer\u001b[38;5;241m.\u001b[39mview_into(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/meshcat/path.py:14\u001b[0m, in \u001b[0;36mPath.append\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         new_path \u001b[38;5;241m=\u001b[39m new_path \u001b[38;5;241m+\u001b[39m (element,)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/meshcat/path.py:4\u001b[0m, in \u001b[0;36mPath.__init__\u001b[0;34m(self, entries)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPath\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;18m__slots__\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentries\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m()):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries \u001b[38;5;241m=\u001b[39m entries\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in range(buffer_size):\n",
    "\n",
    "    if k % n_mpc == 0:\n",
    "        \n",
    "        ## Adding to data only if the kuka reaches the ball\n",
    "        if k > 0:\n",
    "            dist = torch.linalg.norm(dtc(torch.tensor(x_pred[-2*nq:]), model, data, f_id) - x_des)\n",
    "            if dist <= d_tol and error <= 1e-2:\n",
    "                if len(x_train) != 1:\n",
    "                    x_train = torch.vstack((x_train, x_train_tmp))\n",
    "                    y_train = torch.vstack((y_train, y_train_tmp))                \n",
    "                elif len(x_train) == 1:\n",
    "                    x_train = x_train_tmp\n",
    "                    y_train = y_train_tmp\n",
    "                    \n",
    "                    x_train_tmp = torch.zeros_like(x_train_tmp)\n",
    "                    y_train_tmp = torch.zeros_like(y_train_tmp)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            image, label = dl.get_data(np.random.randint(dl.y_train.shape[0]))\n",
    "            pred_loc, encoding = encoder(image)\n",
    "            error = loss(label.unsqueeze(0), pred_loc).numpy()\n",
    "            pred_loc = pred_loc*std + mean\n",
    "            x_des = label*std + mean\n",
    "\n",
    "        viz.viewer[\"box\"].set_object(g.Sphere(0.05), \n",
    "                         g.MeshLambertMaterial(\n",
    "                             color=0xff22dd,\n",
    "                             reflectivity=0.8))\n",
    "        viz.viewer[\"box\"].set_transform(tf.translation_matrix(x_des.detach().numpy()))\n",
    "        \n",
    "        if np.random.randint(3) == 0 or k == 0:\n",
    "            x_init = np.zeros(2*nq)\n",
    "            x_init[0:nq] = q_des_arr[0] + 0.3*2*(np.random.rand(nq) - 0.5)\n",
    "            x_init[0] -= 2*0.5*(np.random.rand(1) - 0.5)\n",
    "            x_init[2] -= 2*0.3*(np.random.rand(1) - 0.5)\n",
    "            x_init[nq:] = 0.7*2*(np.random.rand(nv) - 0.5)\n",
    "    \n",
    "    else:\n",
    "        x_init = x_pred[3*nq*(n_col-1):3*nq*(n_col-1) + 2*nq]\n",
    "    \n",
    "    x_pred = iocfp.predict(x_init[0:nq], x_init[nq:2*nq], x_des)    \n",
    "    \n",
    "    print(\"Index :\" + str(k) + \"/\" + str(buffer_size) + \" Encoder error : \" + str(np.round(error,4)) + \\\n",
    "          \" Data Size is : \" + str(len(x_train)), end = '\\r', flush = True)\n",
    "    \n",
    "    for i in range(n_col+1):\n",
    "        q = x_pred[3*nq*i:3*nq*i + nq]\n",
    "        dq = x_pred[3*nq*i + nq:3*nq*i + 2*nq]\n",
    "\n",
    "        pin.forwardKinematics(model, data, q, dq, np.zeros(nv))\n",
    "        pin.updateFramePlacements(model, data)\n",
    "\n",
    "        viz.display(q)\n",
    "#         time.sleep(0.01)\n",
    "    \n",
    "#     storing x train\n",
    "    x_train_tmp[k % n_mpc][0:2*nq] = torch.tensor(x_init)\n",
    "    x_train_tmp[k % n_mpc][2*nq:] = encoding\n",
    "    \n",
    "#     storing the weights and x_nom\n",
    "    y_train_tmp[k % n_mpc] = torch.hstack((iocfp.ioc.weight.flatten(), iocfp.ioc.x_nom))\n",
    "\n",
    "    if k % 100 == 0 and k:\n",
    "        torch.save(x_train[0:k], \"../data/x_train\" + str(f_name) + \".pt\")\n",
    "        torch.save(y_train[0:k], \"../data/y_train\" + str(f_name) + \".pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

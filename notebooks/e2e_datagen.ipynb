{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2786f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This contains the training procedure for vision to weights\n",
    "## Author : Avadesh Meduri\n",
    "## Date : 31/05/2022\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3e4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "python_path = pathlib.Path('.').absolute().parent/'python'\n",
    "os.sys.path.insert(1, str(python_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de465da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is demo for kuka reaching a desired point with diff_qp\n",
    "## Author : Avadesh Meduri\n",
    "## Date : 25/02/2022\n",
    "import time\n",
    "import numpy as np\n",
    "import pinocchio as pin\n",
    "from robot_properties_kuka.config import IiwaConfig\n",
    "from vocam.diff_pin_costs import DiffFrameTranslationCost, DiffFrameVelocityCost\n",
    "\n",
    "import meshcat\n",
    "import meshcat.transformations as tf\n",
    "import meshcat.geometry as g\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, RandomSampler, Sampler\n",
    "import time\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "import numba\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Resize\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vocam.inverse_qp import IOC\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from vocam.forward_pass import IOCForwardPassWithoutVision\n",
    "from vocam.nets import Net\n",
    "\n",
    "from vocam.qpnet import QPNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a724c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = IiwaConfig.buildRobotWrapper()\n",
    "model, data = robot.model, robot.data\n",
    "f_id = model.getFrameId(\"EE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99124f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7000/static/\n"
     ]
    }
   ],
   "source": [
    "viz = pin.visualize.MeshcatVisualizer(robot.model, robot.collision_model, robot.visual_model)\n",
    "viz.initViewer(open=False)\n",
    "viz.loadViewerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80234186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, fnames, mean = None, std = None, rgbd = True, resize = (224,224)):\n",
    "        \n",
    "        self.rgbd = rgbd\n",
    "        self.resize = resize\n",
    "        self.y_len = [0]\n",
    "        self.img_dir = []\n",
    "        for i in range(len(fnames)):\n",
    "            self.img_dir.append(\"../vision/image_data/data\" + str(fnames[i]))\n",
    "            self.data = np.load(\"../vision/position_data/data\" + str(fnames[i]) + \".npz\")\n",
    "            if i == 0:\n",
    "                self.y_train = torch.tensor(self.data[\"position\"]).float()\n",
    "                self.y_len.append(len(self.data[\"position\"])-1)\n",
    "                \n",
    "            else:\n",
    "                self.y_train = torch.vstack((self.y_train, torch.tensor(self.data[\"position\"]).float()))\n",
    "                self.y_len.append(self.y_len[-1] + len(self.data[\"position\"]))\n",
    "        \n",
    "        if isinstance(mean, np.ndarray) and isinstance(std, np.ndarray):\n",
    "            print(\"using given mean\")\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            self.y_train = (self.y_train - self.mean)/self.std\n",
    "        else:\n",
    "            self.mean = torch.mean(self.y_train, axis = 0)\n",
    "            self.std = torch.std(self.y_train, axis = 0)\n",
    "            print(self.mean, self.std)\n",
    "            self.y_train = (self.y_train - self.mean)/self.std\n",
    "                \n",
    "    def get_data(self, gidx):\n",
    "        \n",
    "        \n",
    "        b_idx = max(np.searchsorted(self.y_len, gidx)-1,0) # which dir to look into\n",
    "        idx = max(gidx - self.y_len[b_idx] - 1,0) # relative idx\n",
    "        \n",
    "#         print(type(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\")))\n",
    "        image = ToTensor()(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\"))\n",
    "        if self.rgbd:\n",
    "            d_image = ToTensor()(imread(self.img_dir[b_idx] + \"/depth_\" + str(idx) + \".jpg\"))\n",
    "            image = torch.vstack((image, d_image))\n",
    "            image = transforms.functional.crop(image,  50, 100, 180, 180)\n",
    "\n",
    "        else:\n",
    "            image = transforms.functional.crop(image,  50, 100, 180, 180)\n",
    "            image = transforms.Resize(self.resize)(image)                    \n",
    "\n",
    "        label = self.y_train[gidx]\n",
    "        \n",
    "        return image.float()[None,:,:,:], label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6dc0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_Net_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv11 = nn.Conv2d(4, 64, 3)\n",
    "        self.conv12 = nn.Conv2d(64, 64, 3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv22 = nn.Conv2d(128, 128, 3)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(128, 256, 3)\n",
    "        self.conv32 = nn.Conv2d(256, 256, 3)\n",
    "        self.conv33 = nn.Conv2d(256, 256, 3)\n",
    "\n",
    "        \n",
    "        self.conv41 = nn.Conv2d(256, 512, 3)\n",
    "        self.conv42 = nn.Conv2d(512, 512, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = self.pool(F.relu(self.conv12(x)))\n",
    "        \n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = self.pool(F.relu(self.conv22(x)))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv31(x)))\n",
    "        x = self.pool(F.relu(self.conv32(x)))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv41(x)))\n",
    "        x = F.relu(self.conv42(x))\n",
    "            \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        enc = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x, enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b9aa639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using given mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq = model.nq\n",
    "nv = model.nv\n",
    "q0 = [np.pi/16.0, -np.pi/16.0, 0, 0, 0, 0, 0]\n",
    "x_init = np.concatenate([q0, pin.utils.zero(model.nv)])\n",
    "\n",
    "n_col = 5\n",
    "u_max = [2.5,2.5,2.5, 1.5, 1.5, 1.5, 1.0]\n",
    "n_vars = 3*nq*n_col+2*nq\n",
    "dt = 0.05\n",
    "\n",
    "isvec = True\n",
    "lr = 1e-1\n",
    "max_eps = 100\n",
    "\n",
    "\n",
    "nn_dir = \"../models/qpnet_89.pt\"\n",
    "net= QPNet(2*nq + 3, 2*n_vars).eval()\n",
    "net.load(nn_dir)\n",
    "\n",
    "iocfp = IOCForwardPassWithoutVision(net, u_max=u_max)\n",
    "\n",
    "dtc = DiffFrameTranslationCost.apply\n",
    "# for the vision part\n",
    "indices = [1,2,3,4,5,6,7,8,9]\n",
    "mean = np.array([0.3068, 0.1732, 0.4015])\n",
    "std = np.array([0.1402, 0.2325, 0.1624])\n",
    "dl = BoxDataSet(indices, mean = mean, std = std, rgbd = True, resize = (224,224))\n",
    "\n",
    "encoder = C_Net_encoder()\n",
    "encoder.load_state_dict(torch.load(\"../vision/models/cnn2\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9b13394",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "f_name = \"e2e_1\"\n",
    "n_mpc = 25\n",
    "d_tol = 0.5 # how close the ee should be to the ball to be accepted as a good data point\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "x_train = torch.zeros((1,len(x_init) + 512))\n",
    "x_train_tmp = torch.zeros((n_mpc,len(x_init) + 512))\n",
    "\n",
    "n_vars = 3*nq*n_col + 2*nq\n",
    "if not isvec:\n",
    "    y_train_tmp = torch.zeros((n_mpc, n_vars**2 + n_vars))\n",
    "else:\n",
    "    y_train_tmp = torch.zeros((n_mpc, 2*n_vars))\n",
    "\n",
    "q_des_arr = np.array([[2.1789238e-02,  3.3214998e-01, -1.4518893e-04, -8.7141126e-01,\n",
    "                          6.0329604e-01, -1.3965217e-03,  1.4794523e-04],\n",
    "                      [1.3737, 0.9711, 1.6139, 1.2188, 1.5669, 0.1236, 0.2565]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfec5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index :10/10000 Encoder error : 0.0001 Data Size is : 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ameduri/pydevel/ioc_qp/python/vocam/forward_pass.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_input = torch.hstack((torch.tensor(state), torch.tensor(x_des))).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index :231/10000 Encoder error : 0.0013 Data Size is : 225\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_449592/1597721168.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mpin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateFramePlacements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m#         time.sleep(0.01)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/openrobots/lib/python3.8/site-packages/pinocchio/visualize/meshcat_visualizer.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhomogeneous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;31m# Update viewer configuration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetViewerNodeName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeometryType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVISUAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisplayCollisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvisibility\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/meshcat/visualizer.py\u001b[0m in \u001b[0;36mset_transform\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSetTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/meshcat/visualizer.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mumsgpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         ])\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzmq_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in range(buffer_size):\n",
    "\n",
    "    if k % n_mpc == 0:\n",
    "        \n",
    "        ## Adding to data only if the kuka reaches the ball\n",
    "        if k > 0:\n",
    "            dist = torch.linalg.norm(dtc(torch.tensor(x_pred[-2*nq:]), model, data, f_id) - x_des)\n",
    "            if dist <= d_tol and error <= 1e-2:\n",
    "                if len(x_train) != 1:\n",
    "                    x_train = torch.vstack((x_train, x_train_tmp))\n",
    "                    y_train = torch.vstack((y_train, y_train_tmp))                \n",
    "                elif len(x_train) == 1:\n",
    "                    x_train = x_train_tmp\n",
    "                    y_train = y_train_tmp\n",
    "                    \n",
    "                    x_train_tmp = torch.zeros_like(x_train_tmp)\n",
    "                    y_train_tmp = torch.zeros_like(y_train_tmp)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            image, label = dl.get_data(np.random.randint(dl.y_train.shape[0]))\n",
    "            pred_loc, encoding = encoder(image)\n",
    "            error = loss(label.unsqueeze(0), pred_loc).numpy()\n",
    "            pred_loc = pred_loc*std + mean\n",
    "            x_des = label*std + mean\n",
    "\n",
    "        viz.viewer[\"box\"].set_object(g.Sphere(0.05), \n",
    "                         g.MeshLambertMaterial(\n",
    "                             color=0xff22dd,\n",
    "                             reflectivity=0.8))\n",
    "        viz.viewer[\"box\"].set_transform(tf.translation_matrix(x_des.detach().numpy()))\n",
    "        \n",
    "        if np.random.randint(2) == 0 or k == 0:\n",
    "            x_init = np.zeros(2*nq)\n",
    "            x_init[0:nq] = q_des_arr[0] + 0.3*2*(np.random.rand(nq) - 0.5)\n",
    "            x_init[0] -= 2*0.5*(np.random.rand(1) - 0.5)\n",
    "            x_init[2] -= 2*0.3*(np.random.rand(1) - 0.5)\n",
    "            x_init[nq:] = 0.7*2*(np.random.rand(nv) - 0.5)\n",
    "    \n",
    "    else:\n",
    "        x_init = x_pred[3*nq*(n_col-1):3*nq*(n_col-1) + 2*nq]\n",
    "    \n",
    "    x_pred = iocfp.predict(x_init[0:nq], x_init[nq:2*nq], x_des)    \n",
    "    \n",
    "    print(\"Index :\" + str(k) + \"/\" + str(buffer_size) + \" Encoder error : \" + str(np.round(error,4)) + \\\n",
    "          \" Data Size is : \" + str(len(x_train)), end = '\\r', flush = True)\n",
    "    \n",
    "    for i in range(n_col+1):\n",
    "        q = x_pred[3*nq*i:3*nq*i + nq]\n",
    "        dq = x_pred[3*nq*i + nq:3*nq*i + 2*nq]\n",
    "\n",
    "        pin.forwardKinematics(model, data, q, dq, np.zeros(nv))\n",
    "        pin.updateFramePlacements(model, data)\n",
    "\n",
    "        viz.display(q)\n",
    "#         time.sleep(0.01)\n",
    "    \n",
    "#     storing x train\n",
    "    x_train_tmp[k % n_mpc][0:2*nq] = torch.tensor(x_init)\n",
    "    x_train_tmp[k % n_mpc][2*nq:] = encoding\n",
    "    \n",
    "#     storing the weights and x_nom\n",
    "    y_train_tmp[k % n_mpc] = torch.hstack((iocfp.ioc.weight.flatten(), iocfp.ioc.x_nom))\n",
    "\n",
    "    if k % 100 == 0 and k:\n",
    "        torch.save(x_train[0:k], \"../data/x_train\" + str(f_name) + \".pt\")\n",
    "        torch.save(y_train[0:k], \"../data/y_train\" + str(f_name) + \".pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec23e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

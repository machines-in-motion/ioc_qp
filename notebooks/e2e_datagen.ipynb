{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2786f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This contains the training procedure for vision to weights\n",
    "## Author : Avadesh Meduri\n",
    "## Date : 31/05/2022\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3e4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "python_path = pathlib.Path('.').absolute().parent/'python'\n",
    "os.sys.path.insert(1, str(python_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de465da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is demo for kuka reaching a desired point with diff_qp\n",
    "## Author : Avadesh Meduri\n",
    "## Date : 25/02/2022\n",
    "import time\n",
    "import numpy as np\n",
    "import pinocchio as pin\n",
    "from robot_properties_kuka.config import IiwaConfig\n",
    "from vocam.diff_pin_costs import DiffFrameTranslationCost, DiffFrameVelocityCost\n",
    "\n",
    "import random\n",
    "import meshcat\n",
    "import meshcat.transformations as tf\n",
    "import meshcat.geometry as g\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, RandomSampler, Sampler\n",
    "import time\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "import numba\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Resize\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vocam.inverse_qp import IOC\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from vocam.forward_pass import IOCForwardPassWithoutVision\n",
    "from vocam.nets import Net\n",
    "\n",
    "from vocam.qpnet import QPNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a724c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = IiwaConfig.buildRobotWrapper()\n",
    "model, data = robot.model, robot.data\n",
    "f_id = model.getFrameId(\"EE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99124f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7000/static/\n"
     ]
    }
   ],
   "source": [
    "viz = pin.visualize.MeshcatVisualizer(robot.model, robot.collision_model, robot.visual_model)\n",
    "viz.initViewer(open=False)\n",
    "viz.loadViewerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80234186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, fnames, mean = None, std = None, rgbd = True, resize = (224,224)):\n",
    "        \n",
    "        self.rgbd = rgbd\n",
    "        self.resize = resize\n",
    "        self.y_len = [0]\n",
    "        self.img_dir = []\n",
    "        for i in range(len(fnames)):\n",
    "            self.img_dir.append(\"../vision/image_data/data\" + str(fnames[i]))\n",
    "            self.data = np.load(\"../vision/position_data/data\" + str(fnames[i]) + \".npz\")\n",
    "            if i == 0:\n",
    "                self.y_train = torch.tensor(self.data[\"position\"]).float()\n",
    "                self.y_len.append(len(self.data[\"position\"])-1)\n",
    "                \n",
    "            else:\n",
    "                self.y_train = torch.vstack((self.y_train, torch.tensor(self.data[\"position\"]).float()))\n",
    "                self.y_len.append(self.y_len[-1] + len(self.data[\"position\"]))\n",
    "        \n",
    "        if isinstance(mean, np.ndarray) and isinstance(std, np.ndarray):\n",
    "            print(\"using given mean\")\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            self.y_train = (self.y_train - self.mean)/self.std\n",
    "        else:\n",
    "            self.mean = torch.mean(self.y_train, axis = 0)\n",
    "            self.std = torch.std(self.y_train, axis = 0)\n",
    "            print(self.mean, self.std)\n",
    "            self.y_train = (self.y_train - self.mean)/self.std\n",
    "                \n",
    "    def get_data(self, gidx):\n",
    "        \n",
    "        \n",
    "        b_idx = max(np.searchsorted(self.y_len, gidx)-1,0) # which dir to look into\n",
    "        idx = max(gidx - self.y_len[b_idx] - 1,0) # relative idx\n",
    "        \n",
    "#         print(type(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\")))\n",
    "        image = ToTensor()(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\"))\n",
    "        if self.rgbd:\n",
    "            d_image = ToTensor()(imread(self.img_dir[b_idx] + \"/depth_\" + str(idx) + \".jpg\"))\n",
    "            image = torch.vstack((image, d_image))\n",
    "            image = transforms.functional.crop(image,  50, 100, 180, 180)\n",
    "\n",
    "        else:\n",
    "            image = transforms.functional.crop(image,  50, 100, 180, 180)\n",
    "            image = transforms.Resize(self.resize)(image)                    \n",
    "\n",
    "        label = self.y_train[gidx]\n",
    "        \n",
    "        return image.float()[None,:,:,:], label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6dc0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_Net_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv11 = nn.Conv2d(4, 64, 3)\n",
    "        self.conv12 = nn.Conv2d(64, 64, 3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv22 = nn.Conv2d(128, 128, 3)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(128, 256, 3)\n",
    "        self.conv32 = nn.Conv2d(256, 256, 3)\n",
    "        self.conv33 = nn.Conv2d(256, 256, 3)\n",
    "\n",
    "        \n",
    "        self.conv41 = nn.Conv2d(256, 512, 3)\n",
    "        self.conv42 = nn.Conv2d(512, 512, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = self.pool(F.relu(self.conv12(x)))\n",
    "        \n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = self.pool(F.relu(self.conv22(x)))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv31(x)))\n",
    "        x = self.pool(F.relu(self.conv32(x)))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv41(x)))\n",
    "        x = F.relu(self.conv42(x))\n",
    "            \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        enc = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x, enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b9aa639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using given mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq = model.nq\n",
    "nv = model.nv\n",
    "q0 = [np.pi/16.0, -np.pi/16.0, 0, 0, 0, 0, 0]\n",
    "x_init = np.concatenate([q0, pin.utils.zero(model.nv)])\n",
    "\n",
    "n_col = 5\n",
    "u_max = [2.5,2.5,2.5, 1.5, 1.5, 1.5, 1.0]\n",
    "n_vars = 3*nq*n_col+2*nq\n",
    "dt = 0.05\n",
    "\n",
    "isvec = True\n",
    "lr = 1e-1\n",
    "max_eps = 100\n",
    "\n",
    "\n",
    "nn_dir = \"../models/qpnet_91.pt\"\n",
    "net= QPNet(2*nq + 3, 2*n_vars).eval()\n",
    "net.load(nn_dir)\n",
    "\n",
    "iocfp = IOCForwardPassWithoutVision(net, u_max=u_max)\n",
    "\n",
    "dtc = DiffFrameTranslationCost.apply\n",
    "# for the vision part\n",
    "indices = [1,2,3,4,5,6,7,11,12,13,14,15,16]\n",
    "mean = np.array([0.2738, 0.1354, 0.4108])\n",
    "std = np.array([0.1312, 0.2271, 0.1631])\n",
    "dl = BoxDataSet(indices, mean = mean, std = std, rgbd = True, resize = (224,224))\n",
    "\n",
    "encoder = C_Net_encoder()\n",
    "encoder.load_state_dict(torch.load(\"../vision/models/cnn4\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b13394",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 2000\n",
    "f_name = \"e2e_3\"\n",
    "n_mpc = 20\n",
    "d_tol = 0.5 # how close the ee should be to the ball to be accepted as a good data point\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "x_train = torch.zeros((1,len(x_init) + 512))\n",
    "x_train_tmp = torch.zeros((n_mpc,len(x_init) + 512))\n",
    "\n",
    "n_vars = 3*nq*n_col + 2*nq\n",
    "if not isvec:\n",
    "    y_train_tmp = torch.zeros((n_mpc, n_vars**2 + n_vars))\n",
    "else:\n",
    "    y_train_tmp = torch.zeros((n_mpc, 2*n_vars))\n",
    "\n",
    "q_des_arr = np.array([[2.1789238e-02,  3.3214998e-01, -1.4518893e-04, -8.7141126e-01,\n",
    "                          6.0329604e-01, -1.3965217e-03,  1.4794523e-04],\n",
    "                      [1.3737, 0.9711, 1.6139, 1.2188, 1.5669, 0.1236, 0.2565]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfec5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index :1999/2000 Encoder error : 0.0152 Data Size is : 1860\r"
     ]
    }
   ],
   "source": [
    "d = random.sample(range(dl.y_train.shape[0]), buffer_size)\n",
    "for k in range(buffer_size):\n",
    "\n",
    "    if k % n_mpc == 0:\n",
    "        \n",
    "        ## Adding to data only if the kuka reaches the ball\n",
    "        if k > 0:\n",
    "            dist = torch.linalg.norm(dtc(torch.tensor(x_pred[-2*nq:]), model, data, f_id) - x_des)\n",
    "            if dist <= d_tol and error <= 1e-2:\n",
    "                if len(x_train) != 1:\n",
    "                    x_train = torch.vstack((x_train, x_train_tmp))\n",
    "                    y_train = torch.vstack((y_train, y_train_tmp))                \n",
    "                elif len(x_train) == 1:\n",
    "                    x_train = x_train_tmp\n",
    "                    y_train = y_train_tmp\n",
    "                    \n",
    "                    x_train_tmp = torch.zeros_like(x_train_tmp)\n",
    "                    y_train_tmp = torch.zeros_like(y_train_tmp)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            image, label = dl.get_data(d[k])\n",
    "            pred_loc, encoding = encoder(image)\n",
    "            error = loss(label.unsqueeze(0), pred_loc).numpy()\n",
    "            pred_loc = pred_loc*std + mean\n",
    "            x_des = label*std + mean\n",
    "            x_des[0] += 0.3 ## added to offset the vicon origin\n",
    "            \n",
    "        viz.viewer[\"box\"].set_object(g.Sphere(0.05), \n",
    "                         g.MeshLambertMaterial(\n",
    "                             color=0xff22dd,\n",
    "                             reflectivity=0.8))\n",
    "        viz.viewer[\"box\"].set_transform(tf.translation_matrix(x_des.detach().numpy()))\n",
    "        \n",
    "        if np.random.randint(3) == 0 or k == 0:\n",
    "            x_init = np.zeros(2*nq)\n",
    "            x_init[0:nq] = q_des_arr[0] + 0.3*2*(np.random.rand(nq) - 0.5)\n",
    "            x_init[0] -= 2*0.5*(np.random.rand(1) - 0.5)\n",
    "            x_init[2] -= 2*0.3*(np.random.rand(1) - 0.5)\n",
    "            x_init[nq:] = 0.7*2*(np.random.rand(nv) - 0.5)\n",
    "    \n",
    "    else:\n",
    "        x_init = x_pred[3*nq*(n_col-1):3*nq*(n_col-1) + 2*nq]\n",
    "    \n",
    "    x_pred = iocfp.predict(x_init[0:nq], x_init[nq:2*nq], x_des)    \n",
    "    \n",
    "    print(\"Index :\" + str(k) + \"/\" + str(buffer_size) + \" Encoder error : \" + str(np.round(error,4)) + \\\n",
    "          \" Data Size is : \" + str(len(x_train)), end = '\\r', flush = True)\n",
    "    \n",
    "    for i in range(n_col+1):\n",
    "        q = x_pred[3*nq*i:3*nq*i + nq]\n",
    "        dq = x_pred[3*nq*i + nq:3*nq*i + 2*nq]\n",
    "\n",
    "        pin.forwardKinematics(model, data, q, dq, np.zeros(nv))\n",
    "        pin.updateFramePlacements(model, data)\n",
    "\n",
    "        viz.display(q)\n",
    "#         time.sleep(0.01)\n",
    "    \n",
    "#     storing x train\n",
    "    x_train_tmp[k % n_mpc][0:2*nq] = torch.tensor(x_init)\n",
    "    x_train_tmp[k % n_mpc][2*nq:] = encoding\n",
    "    \n",
    "#     storing the weights and x_nom\n",
    "    y_train_tmp[k % n_mpc] = torch.hstack((iocfp.ioc.weight.flatten(), iocfp.ioc.x_nom))\n",
    "\n",
    "    if k % 100 == 0 and k:\n",
    "        torch.save(x_train[0:k], \"../data/x_train\" + str(f_name) + \".pt\")\n",
    "        torch.save(y_train[0:k], \"../data/y_train\" + str(f_name) + \".pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017ba90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

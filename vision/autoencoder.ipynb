{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871cf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This file has an autoencoder implementation\n",
    "## Author : Avadesh Meduri\n",
    "## Date : 20/04/2022\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, RandomSampler, Sampler\n",
    "import time\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "import numba\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b1d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, fnames, rgbd = True, resize = (224,224)):\n",
    "        \n",
    "        self.rgbd = rgbd\n",
    "        self.resize = resize\n",
    "        self.y_len = [0]\n",
    "        self.img_dir = []\n",
    "        for i in range(len(fnames)):\n",
    "            self.img_dir.append(\"./image_data/data\" + str(fnames[i]))\n",
    "            self.data = np.load(\"./position_data/data\" + str(fnames[i]) + \".npz\")\n",
    "            if i == 0:\n",
    "                self.y_train = torch.tensor(self.data[\"position\"]).float()\n",
    "                self.y_len.append(len(self.data[\"position\"])-1)\n",
    "                \n",
    "            else:\n",
    "                self.y_train = torch.vstack((self.y_train, torch.tensor(self.data[\"position\"]).float()))\n",
    "                self.y_len.append(self.y_len[-1] + len(self.data[\"position\"]))\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def __getitem__(self, gidx):\n",
    "        \n",
    "        \n",
    "        b_idx = max(np.searchsorted(self.y_len, gidx)-1,0) # which dir to look into\n",
    "        idx = max(gidx - self.y_len[b_idx] - 1,0) # relative idx\n",
    "        \n",
    "        image = ToTensor()(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\"))\n",
    "        if self.rgbd:\n",
    "            d_image = ToTensor()(imread(self.img_dir[b_idx] + \"/depth_\" + str(idx) + \".jpg\"))\n",
    "            image = torch.vstack((image, d_image))\n",
    "        else:\n",
    "            image = transforms.Resize(self.resize)(image)\n",
    "        \n",
    "        label = self.y_train[gidx]\n",
    "        \n",
    "        return image.float(), label\n",
    "                \n",
    "    \n",
    "    def get_data(self, gidx):\n",
    "        \n",
    "        \n",
    "        b_idx = max(np.searchsorted(self.y_len, gidx)-1,0) # which dir to look into\n",
    "        idx = max(gidx - self.y_len[b_idx] - 1,0) # relative idx\n",
    "        \n",
    "        image = ToTensor()(imread(self.img_dir[b_idx] + \"/color_\" + str(idx) + \".jpg\"))\n",
    "        if self.rgbd:\n",
    "            d_image = ToTensor()(imread(self.img_dir[b_idx] + \"/depth_\" + str(idx) + \".jpg\"))\n",
    "            image = torch.vstack((image, d_image))\n",
    "        else:\n",
    "            image = transforms.Resize(self.resize)(image)\n",
    "            \n",
    "        label = self.y_train[gidx]\n",
    "        \n",
    "        return image.float()[None,:,:,:], label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "426784f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 64, 3, padding = (1,1))\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, 3, padding = (1,1))\n",
    "        self.conv3 = nn.Conv2d(32, 16, 3, padding = (1,1))\n",
    "        \n",
    "        self.deconv1 = nn.Conv2d(16, 32, 2, stride=2)\n",
    "        self.deconv2 = nn.Conv2d(32, 64, 2, stride=2)\n",
    "        self.deconv3 = nn.Conv2d(64, 4, 2, stride=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        print(x.shape)\n",
    "        x = self.deconv3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b00fd405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 1\r",
      "loading 2\r",
      "loading 3\r",
      "loading 4\r",
      "loading 5\r"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "net = Encoder()\n",
    "# net.load_state_dict(torch.load(\"./models/cnn1\"))\n",
    "net = net.to(device)\n",
    "lr = 1.5e-4\n",
    "eps = 1000\n",
    "indices = [1,2,3,4,5]\n",
    "num_data = len(indices)\n",
    "start = 1\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = torch.nn.MSELoss() #torch.nn.MSELoss() #torch.nn.HuberLoss()\n",
    "dl_arr = read_data(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72de3387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 30, 53])\n",
      "torch.Size([32, 32, 15, 26])\n",
      "torch.Size([32, 64, 7, 13])\n",
      "torch.Size([32, 4, 3, 6])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (424) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m net(x_train_gpu)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 7\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_gpu\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe iteration number : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The loss is :\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(error\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, flush  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:3261\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3259\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3261\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/functional.py:75\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (424) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "for i in range(eps):\n",
    "    for x_train_batch, y_train_batch in dl_arr[np.random.randint(num_data)]:\n",
    "        x_train_gpu = x_train_batch.to(device)\n",
    "        y_train_gpu = y_train_batch.to(device)\n",
    "        y_pred = net(x_train_gpu)\n",
    "        print(y_pred.shape)\n",
    "        error = loss(y_pred, y_train_gpu) \n",
    "        print(\"The iteration number : \" + str(i) + \" The loss is :\" + str(error.cpu().detach().numpy()), end='\\r', flush  = True)\n",
    "        optimizer.zero_grad()\n",
    "        error.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        torch.save(net.state_dict(), \"./models/enc1\")\n",
    "        \n",
    "torch.save(net.state_dict(), \"./models/enc1\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50638896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
